# **Module 3: Feedforward Neural Networks**

非常基础的神经网络教学。略

# **Module 4: 2D Object Detection**

### **一、2D 目标检测模型**

**输入：**一张驾驶场景的图片。

**输出：**场景内目标的位置：边界框。目标的类别：多分类。

**目标的表示方法：**

使用特征向量$[x_{min}, y_{min}, x_{max}, y_{max}, S_{class1},S_{class_2}...S_{classn}]$表示场景中的一个目标。其中$[x_{min},y_{min}]$表示边界框的左上角，$[x_{max},y_{max}]$表示边界框的右下角（像素坐标系的原点在左上角！）。$[S_{class1},S_{class_2}...S_{classn}]$表示目标对应n种类别的得分。

**模型：**输入驾驶场景的图片，预测场景内所有的目标的边界框和类别。即返回:目标数X$[x_{min}, y_{min}, x_{max}, y_{max}, S_{class1},S_{class_2}...S_{classn}]$的预测结果。

**评价指标：**

- IOU，交并比：评估边界框好坏的指标。设某个目标预测的边界框Pred Box为$B_{pred}$，实际上人为标注的边界框Ground Truth Box为$B_{gt}$则公式为:$\frac{intersection(B_{pred}, B_{gt})}{area(B_{pred}) + area(B_{gt}) - intersection(B_{pred}, B_{gt})}$

  ![image-20210909165956146](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210909165956146.png)

- 评估分类结果的指标：

  - True Positive（TP）: 目标的类别得分>类别阈值 && IOU > IOU阈值

  - False Positive（FP）：目标的类别得分>类别阈值 && IOU < IOU阈值

  - False Negative（FN）：场景没有被模型检测出来的Ground Truth Boxes的数量

  - 查准率(Precision)：TP / (TP + FP)。即预测到的目标中，检测正确的概率

  - 查全率、召回率(Recall)：TP/(TP + FN)。即所有Ground Truth Boxes被检测出来的概率

  - PR曲线：使用不同分类阈值计算得到的查准率P和查全率R的对应关系曲线。

    ![image-20210909171432522](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210909171432522.png)

- Average Precision(AP)：PR曲线与坐标轴围成区域的面积

  ![image-20210909171629428](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210909171629428.png)



**常见问题：**

- 遮挡 Occlusion:前景目标（foreground）的位置遮挡了待检测目标

- 截断 Truncation: 目标的某一部分超出了图片的范围

  ![image-20210909172312762](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210909172312762.png)

- 尺度 Scale:目标距离相机越远，则目标在图片上的成像越小.

- 光照illumination...

  

### **二、CNN与2D目标识别**

#### **整体框架：**

![image-20210909182749974](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210909182749974.png)

应用于2D目标检测的基本CNN架构如上。模型**输入驾驶场景的图片和锚框** （Anchor Boxes 、Prior Boxes）。首先使用**特征提取器**（CNN网络）提取图片的特征图，然后将特征图和锚框输入到**输出层**：在这里模型将结合特征图和锚点、锚框，使用各种方法从锚框中提取出感兴趣区域ROIs。最后使用**非极大值抑制（NMS）**来进一步筛选预测的Boxes。

#### **特征提取器（略）：**

VGG、Resnet等CNN模型。原始图片经过这些模型，一般都会减小尺度(长和宽)，但是会增加通道数。记住这里特征图相对与原始图片缩小的**尺寸倍率**，这将用于后面特征图和原始图片的映射关系。

![image-20210909184450010](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210909184450010.png)

#### **锚点、锚框：**

![image-20210909190006553](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210909190006553.png)

- 什么是锚点、锚框：

  锚点是用坐标$[u,v]$表示的**特征图**上的一个点。以这个点为中心和$[l，w]$为矩形的长和宽绘制矩形框，则$[u,v,l,w]$就可以表示为一个锚框。此外还可以加上矩形框和x轴的夹角，即$[u,v,l,w,r]$可以表示一个**旋转的锚框**。

- 锚框的参考对象：

  一般而言，在特征图上取锚点。但是因为要与标签计算IOU，却要**在原始图片上表示锚框**。因此需要将特征图上的锚点映射到原始图片上去。设特征图上锚点坐标$[u_f,v_f]$，经过特征提取器后原始图片尺寸缩小了n倍。则原始图片上锚点的坐标为$[u_i=n*u_f,v_i=n*v_f]$

  当然如果直接在原始图片上取锚点，就不用担心上面的问题了。

- 为什么要使用锚点？

  目标检测模型的一个通用的步骤是：在原始图片上划分出待检测区域，然后通过计算这些区域和真实标签的IOU，来获取感兴趣区域ROIs(Region of Interests)，最终使用ROIs预测目标的Boxes。因此如何科学高效地划分待检测区域就成一个问题，比如最直接的方法是使用固定的窗口和滑动幅度划分图片，或者使用类似于R-CNN模型中基于手工图像特征划分的SS算法（Selective Search）以上的方法计算量都很大，不能做到实时推断。 而基于锚点的目标检测模型的待检测区域就是通过锚点和锚框确定的，并通过CNN网络从中提取ROIs。最简单的确定锚点的方法是**认为原始图片上的每个像素点都是锚点**，然后给定一组矩形的长和宽，最终得到一系列锚框，最后从这些锚框中提取ROIs。这种方案可行，但是计算量还是很大。

  已知由CNN提取图片的特征图中，特征图的**长宽会下降**且每个像素点放缩到原始图片上对应的是一个区域。即CNN提取的特征图**增加**了每个像素的**感受野**。如果**认为特征图上的每一个像素都是锚点**，最终在原始图片上得到的锚框数量要比在原图片上穷举的方法少很多，且锚框对图像每个区域的覆盖度也不会差。



#### **输出层：**

输出层的构造是各种2D目标检测器中最为核心的部分。这里以**[Faster R-CNN模型](https://arxiv.org/abs/1506.01497)**的输出层：包含 **RPN + ROI pooling层**为例。整个Faster R-CNN模型图示如下：

![image-20210910144419557](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210910144419557.png)

**RPN网络、提取感兴趣区域：**

RPN网络源于Faster R-CNN模型，目的在于使用**特征图上的锚点确定的锚框和事先标定的Ground Truth Boxes**找出哪些锚框是感兴趣区域，以及初步判别这些锚框对应区域中有没有目标，即区分前景和背景。

**两个预处理**

在做进一步的处理之前。用$[u_i,v_i,l,w]$表示一个原始图片上的一个锚框，将这个锚框与人工事先标注的N个Ground Truth Boxes计算IOU。对于每个Ground Truth Box，规定：

- 与其IOU最高的锚框分为正类，用1表示。（保证每个Ground Truth Box至少有一个正类的锚框）
- 与其IOU大于0.7的锚框分为正类，用1表示。
- 与其IOU小于0.3的锚框分为负类，用0表示。
- IOU在0.3~0.7之间的锚框不在之后的流程中被考虑，直接丢弃。

此外另一个处理就是计算**正类锚框**与其对应的ground truth之间的偏移量$[\Delta u, \Delta v, \Delta l, \Delta w]$(每个正类锚框只可能有一个ground truth Box与其对应，且并不关心负类锚框的偏移量)。其公式如下:
$$
\Delta u = \frac{u_{gt} - u_{anchor}}{u_{anchor}}\ \ \ \
\Delta v = \frac{v_{gt} - v_{anchor}}{v_{anchor}}\\
\Delta l = \frac{l_{gt}}{l_{anchor}}\ \ \ \
\Delta w = \frac{w_{gt}}{w_{anchor}}\\
$$
经过上面的两个预处理后，每个锚框额外附带了分类信息以及每个正类锚框相对于ground truth的偏移信息，即用$[u_i,v_i,l,w,c，\Delta u, \Delta v, \Delta l, \Delta w]$表示原始图片上的一个锚框以及框内是否有目标和框位置的精准度。（注意此时的分类信息，仅仅只是区分锚框内有没有目标，并不做目标类别的区分）



**RPN网络的损失函数、训练RPN**

![image-20210910144454717](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210910144454717.png)



获得每个锚框的分类信息和偏移信息之后。如上图，假设为每个锚点分配k个尺寸的锚框。以特征图的每个像素为中心，做**周围3x3范围内的卷积**（其实特征图中的每个像素、锚点已经是256维的向量，原作者在这里使用3x3的卷积意图是融合周围的特征？）。得到1x1x256维度的输出特征，然后使用**1x1的卷积**核融合256维度的通道信息（作者在这就是单纯的用1x1卷积将256D的特征降维到2\*k维代表k个锚框的分类得分，以及4\*k代表k个锚框的偏移量）。最终每个锚框分别得到**2维的分类得分**，和**4维的用于Box回归的偏移量**。

- 分类得分：

  二维的分类得分，一个代表预测锚框为前景（正例，即包含目标）的得分。另一个代表预测锚框为背景（负例，即不含目标）的得分。而之前已经为每个锚框标注了类别（0或1）。因此分类得分和真实标注直接的差别就是二分类损失，根据二分类损失可以**训练区分锚框是否包含目标的分类器**。

- Box回归的偏移量：

  期望预测的4维偏移量$[\Delta \hat u, \Delta \hat v, \Delta \hat l, \Delta \hat w]$和真实的偏移量$[\Delta u, \Delta v, \Delta l, \Delta w]$之间的差距越小越好。这属于回归问题，使用各种回归损失，比如Faster R-CNN使用Smooth-L1损失，可以训练用于**预测Box偏移量的回归器**。

设$p_i$为第$i$个锚框内包含目标的概率，$p_i^*$为该锚框真实的类别(1或0)。$t_i$为第$i$个锚框预测的偏移量，$t_i^*$为其真实的偏移量。定义二分类损失函数$L_{cls}(p_i, p_i^*)$表示第$i$个锚框的分类损失，定义回归损失函数$P_i^*L_{reg}(t_i,t_i^*)$表示其回归损失（注意正例$p_i^*$=1，而负例$p_i^*$=0。因此负例的回归损失默认就是0）。则一个Batch下的总损失为：
$$
L(\{p_i\}\{t_i\})=\frac{1}{N_{cls}}\sum_iL_{cls}(p_i, p_i^*)+\lambda\frac{1}{N_{reg}}\sum_iP_i^*L_{reg}(t_i,t_i^*)
$$
其中$N_{cls}$=batch_size，而$N_{reg}$为一个batch中全部锚点的数量（不是锚框！）。$\lambda$为权重系数。

注：

​	Faster-R-CNN的回归损失函数为Smooth-L1损失，表示为$G(t_i - t_i^*)$。其中：
$$
G
\begin{cases}
0.5x^2 &\abs{x}\le1\\
\abs{x}-0.5 &otherwise
\end{cases}
$$
**ROI Polling：统一ROIs的尺寸**

经过RPN得到的感兴趣区域ROIs的尺寸并不是统一的。这就导致无法使用统一的模型来批量处理这些ROIs。为此需要使用一种特殊的Pooling方法。且RPN网络输出的ROIs是基于原始图片的。如果要提取它们的特征又需要使用CNN网络。为了减少这一计算开销，直接将基于原始图片的ROIs按比例（下采样倍数）映射到特征图上构成基于特征图的ROIs。此后的ROIs默认为基于特征图的ROIs。

pooling:

poling的方法就是直接按照预想的输出尺寸尽量等分每一个ROI，然后在每一个等分的的区域中做maxpoling。假设，预想的输出为2 x 2的特征图，原ROI大小为5 x 7。

- 第一步，尽量等分ROI。即原ROI的5分为2，3；7分为3，4。形成4个区域，大小分别为（2，3）、（2，4）、（3，3）、（3，4）。

- 第二步，在每个区域内做maxpolling。即最终输出2 x 2的特征图。

  动图如下：

  ![20180511113933913](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/20180511113933913.gif)

**最终输出：**

假设RPN输出了N个基于原始图片的ROI，经过ROI Pooling后所有ROIs都是基于特征图的，且大小为 channels x n x m 。即最终输出了形状为(N, c, n, m)的特征图，如果算上batch size就是(B,N,c,n,m)。这样的输出可以直接交给后面的网络进行进一步的处理。比如通过MLP + Softmax层计算每个ROI中的目标的具体类别；通过MLP层输出4个数值，使用回归模型进一步修正预测的Boxes的偏移系数等。

**非极大值抑制（NMS）：**

非极大值抑制目的是：减少冗余的目标检测框。输入包含目标检测框的位置信息和他们的分类得分的列表。循环以下过程：

- 将列表中得分最高的Box放入输出列表中，并将其从输入列表中删除。
- 计算输入列表中剩余Boxes与该得分最高Box的IOU，设置一个阈值。将IOU高于阈值的Boxes直接删除。
- 重复上面两个步骤，直到输出列表中没有Box为止

由于NMS是针对目标检测框的，所以在任何输出检测框的网络后面都可以尝试加入NMS处理。其中最基础的用法就是在任何目标检测算法的最终输出后面进行NMS操作，能够筛除一些重复包含目标的预测框。也可以对RPN网络的输出进行NMS操作（注意此时的分类得分是锚框内有没有目标），这样就可以减少ROIs的数量，提高模型的训练或推断速度。

**TO SUM UP：**

- 使用特征提取器提取原始图片的特征图。这个特征图将会多次被利用，称为**全局特征图**。
- 在**全局特征图上取锚点，在原始图片上定锚框**。与原始图片的Ground Truth Boxes计算IOU，按照规则对锚框进行二分类。并计算正类锚框与唯一关联的Ground Truth Box的4个偏移量。为锚框打上类别和偏移量的标签。
- 使用锚点和**带标签的锚框**，基于全局特征图训练RPN网络。输出感兴趣区域ROIs。
- 使用ROI Pooling统一由ROI生成的特征图的尺寸，称为ROI 特征图。
- 基于ROI特征图进行进一步的分类、Boxes回归或其他操作...



### **三、2D目标检测和自动驾驶**

#### **2D目标检测-->3D目标检测**

**3D目标：**

用$[u,v,l,w,r]$表示2D的目标检测框，当扩展到3维时。用目标框的中心位置$(x, y,z)$，目标框的尺寸$(l,w,h)$以及目标框的与三个坐标轴的夹角$(\phi,\psi,\theta)$来表示目标框的位置信息。其中在自动驾驶场景中中多关心车辆的航向角（yaw angel）即$\theta$。因此3D目标框的位置信息可以用$[x,y,z,l,w,h,\theta]$来表示。

![image-20210913144550797](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210913144550797.png)

**2D-->3D:基于视锥（Frustum）（略）**

基于2D目标检测扩展到3D目标检测的有很多思路。这里以基于视锥的方法为例，代表模型：[Frustum PointNets](https://arxiv.org/abs/1711.08488)。

给定2D的目标检测框和场景的点云数据，通过以下方法将2D目标检测框扩展到3D目标检测框。

- 使用点云坐标系到像素坐标系的投影矩阵的逆矩阵或伪逆矩阵，将2D目标检测框的4个顶点投影到点云坐标系中去。因为单独的图片无法恢复深度信息，因此4个顶点在点云坐标系下会形成以相机位置为交点的**4条视线**(view ray)，如下图所示：

  ![20190722233756878](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/20190722233756878.jpg)

  

  ![image-20210913150726368](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210913150726368.png)

- 可以人为待检测的3D目标一定在这4条视线构成的视锥里。为此需要使用某些方法从这些视锥内的点云中提取出目标的深度信息。

- Frustum PointNets中先使用PointNet对视锥中的点云进行点云级别的实例分割。然后基于实例分割的结果在使用类PointNet模型预测3D目标的位置信息$[x,y,z,l,w,h,\theta]$。实际上就是分别使用了PointNet的点云分割框架和点云分类框架。

  ![image-20210913152852827](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210913152852827.png)

#### **2D目标检测-->2D目标跟踪（略）**

目标跟踪(object tracking)不等于将连续多帧目标检测的结果串联起来！目标检测是实现目标跟踪的一个基础。

基于检测的目标跟踪流程：

- 首先确定一个运动模型，比如$p_k=p_{k-1}+v_k\Delta t+\mathcal N(0,1)$

- 给出运动场景的第一帧，识别其中的目标。根据运动模型预测（prediction）目标在下一帧的位置
- 识别下一帧中的目标。作为此帧的测量结果（Mesurement Bounding Boxes）。
- 对每一个预测框，计算测量框与预测框的IOU。选择与其IOU最高的测量框作为关联（correlation）的对象，形成[预测，测量]对。
- 根据此帧关联的预测prediction和测量结果mesurement，使用卡尔曼滤波(Kalman Filter)更新运动模型。
- 重复执行以上的prediction-mesurement-correlation-update的过程

注意事项：

- 对于每一帧，如果测量框没有与之关联的预测框。代表这个跟踪目标是新增的，将其加入到跟踪队列。
- 如果在当前帧，预测框没有与之关联的测量框。代表这个跟踪目标在当前帧没有出现在场景内，将其从跟踪队列中剔除，终止对其的跟踪。（当然有的模型能够丢失目标后再找回目标）

- 上面的方法能被应用与3D目标检测到3D目标跟踪。

